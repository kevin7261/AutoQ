import os  # åŒ¯å…¥ä½œæ¥­ç³»çµ±æ¨¡çµ„ï¼Œç”¨æ–¼è™•ç†æª”æ¡ˆè·¯å¾‘ã€è®€å–ç’°å¢ƒè®Šæ•¸ç­‰
import shutil  # åŒ¯å…¥é«˜éšæª”æ¡ˆæ“ä½œæ¨¡çµ„ï¼Œç”¨æ–¼è¤‡è£½ã€ç§»å‹•æˆ–åˆªé™¤æª”æ¡ˆèˆ‡ç›®éŒ„
import zipfile  # åŒ¯å…¥ ZIP å£“ç¸®æª”è™•ç†æ¨¡çµ„ï¼Œç”¨æ–¼è§£å£“ç¸®èˆ‡å£“ç¸®æª”æ¡ˆ
import uuid  # åŒ¯å…¥ UUID æ¨¡çµ„ï¼Œç”¨æ–¼ç”¢ç”Ÿå”¯ä¸€è­˜åˆ¥ç¢¼ (Task ID)ï¼Œé¿å…å¤šäººä½¿ç”¨æ™‚æª”åè¡çª
import json  # [æ–°å¢] åŒ¯å…¥ JSON æ¨¡çµ„ï¼Œç”¨æ–¼è§£æ OpenAI å›å‚³çš„ JSON å­—ä¸²
from typing import List, Optional  # [ä¿®æ”¹] åŒ¯å…¥ Optional ç”¨æ–¼æ¨™è¨˜å¯é¸åƒæ•¸

# åŒ¯å…¥ FastAPI ç›¸é—œå…ƒä»¶
from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks, HTTPException  # [æ–°å¢] HTTPException ç”¨æ–¼éŒ¯èª¤è™•ç†
from fastapi.responses import FileResponse, JSONResponse  # åŒ¯å…¥å›æ‡‰é¡åˆ¥ï¼Œåˆ†åˆ¥ç”¨æ–¼å›å‚³æª”æ¡ˆèˆ‡ JSON è³‡æ–™
from fastapi.middleware.cors import CORSMiddleware  # åŒ¯å…¥ CORS (è·¨ä¾†æºè³‡æºå…±äº«) ä¸­ä»‹è»Ÿé«”ï¼Œè§£æ±ºè·¨ç¶²åŸŸè«‹æ±‚å•é¡Œ
from pydantic import BaseModel # [æ–°å¢] ç”¨æ–¼å®šç¾©è³‡æ–™æ¨¡å‹

# --- [æ–°å¢] OpenAI åŸç”Ÿå®¢æˆ¶ç«¯ ---
from openai import OpenAI  # ç”¨æ–¼ç›´æ¥å‘¼å« GPT-4o æ¨¡å‹ API

# --- LangChain & OpenAI ç›¸é—œå¥—ä»¶ ---
# å¾ LangChain ç¤¾ç¾¤å¥—ä»¶åŒ¯å…¥å„ç¨®æ–‡ä»¶è®€å–å™¨
from langchain_community.document_loaders import (
    PyPDFLoader,    # ç”¨æ–¼è®€å– PDF æª”æ¡ˆ
    Docx2txtLoader, # ç”¨æ–¼è®€å– Word (.docx) æª”æ¡ˆ
    TextLoader,     # ç”¨æ–¼è®€å–ç´”æ–‡å­—æª”æ¡ˆ (.txt, .md, .py ç­‰)
    BSHTMLLoader    # ç”¨æ–¼è®€å– HTML ç¶²é æª”æ¡ˆ
)
# åŒ¯å…¥æ–‡å­—åˆ‡åˆ†å™¨ï¼Œç”¨æ–¼å°‡é•·æ–‡ä»¶åˆ‡æˆå°å¡Šï¼Œé€™æ˜¯ RAG çš„é—œéµæ­¥é©Ÿ
from langchain_text_splitters import RecursiveCharacterTextSplitter
# åŒ¯å…¥ OpenAI çš„ Embeddings (å‘é‡åŒ–å·¥å…·) èˆ‡ ChatOpenAI (èŠå¤©æ¨¡å‹)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# åŒ¯å…¥ FAISS å‘é‡è³‡æ–™åº«ï¼Œç”¨æ–¼å„²å­˜èˆ‡æœå°‹å‘é‡ (é€™æ˜¯æˆ‘å€‘ RAG Zip çš„æ ¸å¿ƒæ ¼å¼)
from langchain_community.vectorstores import FAISS
# åŒ¯å…¥ LangChain çš„åŸºç¤æ–‡ä»¶ç‰©ä»¶çµæ§‹
from langchain_core.documents import Document
# åŒ¯å…¥æª¢ç´¢å•ç­”éˆ (RetrievalQA)ï¼Œé€™æ˜¯ä¸²æ¥æª¢ç´¢èˆ‡ç”Ÿæˆçš„æ¨™æº–æµç¨‹
from langchain.chains import RetrievalQA
# åŒ¯å…¥æç¤ºæ¨¡æ¿ (PromptTemplate)ï¼Œç”¨æ–¼è‡ªè¨‚ AI çš„è§’è‰²èˆ‡æŒ‡ä»¤
from langchain.prompts import PromptTemplate

# åˆå§‹åŒ– FastAPI æ‡‰ç”¨ç¨‹å¼å¯¦ä¾‹
app = FastAPI()

# è¨­å®š CORS (è·¨ä¾†æºè³‡æºå…±äº«) ä¸­ä»‹è»Ÿé«”
# é€™è®“å‰ç«¯ç¶²é  (å³ä½¿ä¸åŒç¶²åŸŸï¼Œä¾‹å¦‚ GitHub Pages) ä¹Ÿå¯ä»¥å‘¼å«é€™å€‹ API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è¨±æ‰€æœ‰ä¾†æºçš„ç¶²åŸŸå­˜å– (ç”Ÿç”¢ç’°å¢ƒå»ºè­°è¨­å®šç‰¹å®šç¶²åŸŸä»¥ç­–å®‰å…¨)
    allow_methods=["*"],  # å…è¨±æ‰€æœ‰ HTTP æ–¹æ³• (å¦‚ GET, POST ç­‰)
    allow_headers=["*"],  # å…è¨±æ‰€æœ‰ HTTP æ¨™é ­
)

# --- è¨­å®šæš«å­˜ç›®éŒ„çµæ§‹ ---
# å®šç¾©åŸºç¤æš«å­˜ç›®éŒ„åç¨±ï¼Œæ‰€æœ‰æ“ä½œéƒ½æœƒåœ¨é€™å€‹è³‡æ–™å¤¾å…§é€²è¡Œ
BASE_TEMP_DIR = "temp_rag_processing"
# å®šç¾©ä¸Šå‚³æª”æ¡ˆçš„å­˜æ”¾ç›®éŒ„
UPLOAD_DIR = os.path.join(BASE_TEMP_DIR, "uploads")
# å®šç¾©è§£å£“ç¸®å¾Œçš„æª”æ¡ˆå­˜æ”¾ç›®éŒ„
EXTRACT_DIR = os.path.join(BASE_TEMP_DIR, "extracted")
# å®šç¾©è™•ç†å®Œæˆçš„è¼¸å‡ºæª”æ¡ˆ (å¦‚å‘é‡åº« Zip) å­˜æ”¾ç›®éŒ„
OUTPUT_DIR = os.path.join(BASE_TEMP_DIR, "outputs")

# æª¢æŸ¥ä¸Šè¿°ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡è‡ªå‹•å»ºç«‹
for d in [UPLOAD_DIR, EXTRACT_DIR, OUTPUT_DIR]:
    os.makedirs(d, exist_ok=True)  # exist_ok=True è¡¨ç¤ºè‹¥ç›®éŒ„å·²å­˜åœ¨å‰‡ä¸å ±éŒ¯ï¼Œé¿å…ç¨‹å¼ä¸­æ–·

# --- è¼”åŠ©å‡½æ•¸ï¼šæ¸…ç†æª”æ¡ˆ ---
def cleanup_files(paths_to_remove: List[str], dirs_to_remove: List[str]):
    """
    èƒŒæ™¯ä»»å‹™å‡½æ•¸ï¼šç”¨æ–¼ API å›æ‡‰å¾Œæ¸…ç†æš«å­˜æª”æ¡ˆèˆ‡è³‡æ–™å¤¾
    é€™æ˜¯ä¸€å€‹é‡è¦çš„ç¶­è­·åŠŸèƒ½ï¼Œé¿å…ä¼ºæœå™¨ç¡¬ç¢Ÿç©ºé–“è¢«æš«å­˜æª”å¡æ»¿
    """
    # éæ­·éœ€è¦åˆªé™¤çš„å–®ä¸€æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    for path in paths_to_remove:
        if os.path.exists(path):  # å…ˆæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            try:
                os.remove(path)  # å˜—è©¦åˆªé™¤æª”æ¡ˆ
            except Exception as e:  # å¦‚æœåˆªé™¤å¤±æ•— (ä¾‹å¦‚æª”æ¡ˆè¢«ä½”ç”¨)
                print(f"Error removing file {path}: {e}")  # å°å‡ºéŒ¯èª¤è¨Šæ¯ä½†ä¸ä¸­æ–·ç¨‹å¼

    # éæ­·éœ€è¦åˆªé™¤çš„è³‡æ–™å¤¾è·¯å¾‘åˆ—è¡¨
    for dir_path in dirs_to_remove:
        if os.path.exists(dir_path):  # å…ˆæª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
            try:
                shutil.rmtree(dir_path, ignore_errors=True)  # éè¿´åˆªé™¤è³‡æ–™å¤¾åŠå…¶å…§å®¹ (rm -rf çš„æ•ˆæœ)
            except Exception as e:  # å¦‚æœåˆªé™¤å¤±æ•—
                print(f"Error removing dir {dir_path}: {e}")  # å°å‡ºéŒ¯èª¤è¨Šæ¯

# --- è¼”åŠ©å‡½æ•¸ï¼šè®€å–å–®ä¸€æª”æ¡ˆ ---
def load_single_file(file_path: str) -> List[Document]:
    """
    æ ¹æ“šæª”æ¡ˆçš„å‰¯æª”åï¼Œé¸æ“‡å°æ‡‰çš„ LangChain Loader ä¾†è®€å–å…§å®¹
    å›å‚³ä¸€å€‹ Document ç‰©ä»¶åˆ—è¡¨
    """
    # å–å¾—æª”æ¡ˆå‰¯æª”åä¸¦è½‰ç‚ºå°å¯«ï¼Œæ–¹ä¾¿åˆ¤æ–·
    ext = os.path.splitext(file_path)[1].lower()
    try:
        # åˆ¤æ–·æ˜¯å¦ç‚º PDF
        if ext == ".pdf":
            loader = PyPDFLoader(file_path)
        # åˆ¤æ–·æ˜¯å¦ç‚º Word æª” (.docx)
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
        # åˆ¤æ–·æ˜¯å¦ç‚ºç¨‹å¼ç¢¼æˆ–ç´”æ–‡å­—æª” (å¢åŠ æ”¯æ´ R, Rmd, Py, Md, Txt)
        elif ext in [".txt", ".md", ".r", ".rmd", ".py"]:
            # ä½¿ç”¨ TextLoaderï¼Œä¸¦é–‹å•Ÿè‡ªå‹•ç·¨ç¢¼åµæ¸¬ (autodetect_encoding) ä»¥é¿å…ä¸­æ–‡äº‚ç¢¼
            loader = TextLoader(file_path, encoding="utf-8", autodetect_encoding=True)
        # åˆ¤æ–·æ˜¯å¦ç‚ºç¶²é æª”
        elif ext in [".html", ".htm"]:
            # ä½¿ç”¨ BSHTMLLoader (BeautifulSoup) è§£æ HTML çµæ§‹
            loader = BSHTMLLoader(file_path, open_encoding="utf-8")
        else:
            # å¦‚æœæ˜¯ä¸æ”¯æ´çš„æ ¼å¼ (å¦‚ jpg, xlsx)ï¼Œå›å‚³ç©ºåˆ—è¡¨ï¼Œç¨‹å¼æœƒè‡ªå‹•ç•¥é
            return []
        # åŸ·è¡Œè®€å–ä¸¦å›å‚³æ–‡ä»¶å…§å®¹
        return loader.load()
    except Exception as e:
        # å¦‚æœè®€å–éç¨‹ç™¼ç”ŸéŒ¯èª¤ (å¦‚æª”æ¡ˆææ¯€)ï¼Œå°å‡ºéŒ¯èª¤ä¸¦å›å‚³ç©ºåˆ—è¡¨ï¼Œç¢ºä¿ä¸»ç¨‹å¼ä¸å´©æ½°
        print(f"âš ï¸ ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")
        return []

# --- å…±é€šå‡½æ•¸ï¼šè™•ç† ZIP ä¸¦å›å‚³ Documents (ç”¨æ–¼è™•ç†åŸå§‹æ–‡ä»¶ Zip) ---
def process_zip_to_docs(zip_path, extract_path):
    """
    è§£å£“ç¸® ZIP æª”ï¼Œä¸¦éè¿´æƒæç›®éŒ„ï¼Œè®€å–æ‰€æœ‰æ”¯æ´çš„åŸå§‹æ–‡ä»¶
    å›å‚³æ‰€æœ‰è®€å–åˆ°çš„ Document ç‰©ä»¶åˆ—è¡¨
    """
    # æª¢æŸ¥è©²è·¯å¾‘æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ ZIP æª”
    if not zipfile.is_zipfile(zip_path):
        raise ValueError("ç„¡æ•ˆçš„ ZIP æª”")

    # é–‹å•Ÿ ZIP æª”ä¸¦è§£å£“ç¸®åˆ°æŒ‡å®šç›®éŒ„ (extract_path)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

    all_documents = []  # ç”¨æ–¼å­˜æ”¾æ‰€æœ‰è®€å–åˆ°çš„æ–‡ä»¶
    # å®šç¾©æ”¯æ´çš„å‰¯æª”ååˆ—è¡¨
    supported_exts = ('.pdf', '.docx', '.txt', '.md', '.py', '.html', '.r', '.rmd')

    # ä½¿ç”¨ os.walk éè¿´éæ­·è§£å£“ç¸®å¾Œçš„ç›®éŒ„ (åŒ…å«å­è³‡æ–™å¤¾)
    for root, _, files in os.walk(extract_path):
        for filename in files:
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºæ”¯æ´çš„æ ¼å¼
            if filename.lower().endswith(supported_exts):
                # çµ„åˆå®Œæ•´çš„æª”æ¡ˆè·¯å¾‘
                file_path = os.path.join(root, filename)
                # å‘¼å« load_single_file è®€å–è©²æª”æ¡ˆ
                docs = load_single_file(file_path)

                # ç‚ºè®€å–åˆ°çš„æ–‡ä»¶åŠ å…¥ Metadata (å…ƒæ•¸æ“š)ï¼Œé€™å° RAG æº¯æºå¾ˆé‡è¦
                # è¨ˆç®—ç›¸å°è·¯å¾‘ (ä¾‹å¦‚: "subfolder/doc.pdf")
                rel_path = os.path.relpath(file_path, extract_path)
                for d in docs:
                    d.metadata["filename"] = filename  # ç´€éŒ„æª”å
                    d.metadata["source"] = rel_path    # ç´€éŒ„ç›¸å°è·¯å¾‘ä¾†æº
                # å°‡è™•ç†å¥½çš„æ–‡ä»¶åŠ å…¥ç¸½åˆ—è¡¨
                all_documents.extend(docs)

    return all_documents

# --- [æ–°å¢] è¼”åŠ©å‡½æ•¸ï¼šå–å¾— OpenAI Client ---
def get_openai_client():
    """
    åˆå§‹åŒ–ä¸¦å›å‚³ OpenAI åŸç”Ÿå®¢æˆ¶ç«¯ï¼Œç”¨æ–¼æ–° API ç›´æ¥å‘¼å«æ¨¡å‹
    """
    api_key = os.getenv("OPENAI_API_KEY")  # å¾ç’°å¢ƒè®Šæ•¸è®€å– Key
    if not api_key:
        raise ValueError("æœªè¨­å®š OPENAI_API_KEY")  # è‹¥ç„¡ Key å‰‡å ±éŒ¯
    return OpenAI(api_key=api_key)  # å›å‚³ Client ç‰©ä»¶

# --- [æ–°å¢] è¼”åŠ©å‡½æ•¸ï¼šå¾è§£å£“ç¸®ç›®éŒ„å–å¾— GIS æª”æ¡ˆåç¨± ---
def get_gis_filenames(extract_path) -> List[str]:
    """
    æƒæç›®éŒ„ï¼Œæ‰¾å‡º .shp, .csv ç­‰å¯ç”¨æ–¼ GIS å¯¦ä½œçš„æª”æ¡ˆåç¨±
    é€™å°‡ç”¨æ–¼æä¾›çµ¦ AIï¼Œè®“å®ƒçŸ¥é“æœ‰å“ªäº›ç´ æå¯ä»¥å‡ºé¡Œ
    """
    gis_exts = ('.shp', '.csv', '.tif', '.tiff', '.geojson', '.txt', '.json', '.kml')  # å®šç¾©æ„Ÿèˆˆè¶£çš„å‰¯æª”å
    found_files = []  # åˆå§‹åŒ–çµæœåˆ—è¡¨
    for root, _, files in os.walk(extract_path):  # éæ­·ç›®éŒ„
        for filename in files:
            if filename.lower().endswith(gis_exts):  # å¦‚æœç¬¦åˆ GIS æ ¼å¼
                found_files.append(filename)  # åŠ å…¥åˆ—è¡¨
    return found_files  # å›å‚³æª”ååˆ—è¡¨

# å®šç¾©æ ¹è·¯å¾‘ (Root Endpoint)
@app.get("/")
def home():
    # å›å‚³ç°¡å–®çš„ JSON è¨Šæ¯ï¼Œç¢ºèªä¼ºæœå™¨æ­£åœ¨é‹ä½œï¼Œä¸¦å‘ŠçŸ¥å¯ç”¨çš„ API è·¯å¾‘
    return {"message": "RAG Server Ready. Endpoints: /process_zip, /ask_with_zip, /api/generate_question, /api/grade_submission"}

# =========================================================
# åŠŸèƒ½ 1: è£½ä½œä¸¦ä¸‹è¼‰ Vector DB (åŸå§‹æ–‡ä»¶ -> RAG Zip)
# =========================================================
@app.post("/process_zip")
async def process_zip_file(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """
    æ¥æ”¶åŸå§‹æ–‡ä»¶ Zipï¼Œè£½ä½œæˆ FAISS å‘é‡è³‡æ–™åº«ï¼Œä¸¦å›å‚³ Zip ä¾›ä½¿ç”¨è€…ä¸‹è¼‰ã€‚
    """
    # å¾ç’°å¢ƒè®Šæ•¸å–å¾— OpenAI API Keyï¼Œé€™æ˜¯å‘¼å« Embedding æ¨¡å‹å¿…éœ€çš„
    api_key = os.getenv("OPENAI_API_KEY")
    # å¦‚æœæ²’æœ‰è¨­å®š API Keyï¼Œå›å‚³ 500 éŒ¯èª¤
    if not api_key:
        return JSONResponse(status_code=500, content={"error": "æœªè¨­å®š OPENAI_API_KEY"})

    # ç”¢ç”Ÿä¸€å€‹å”¯ä¸€çš„ Task IDï¼Œç”¨æ–¼éš”é›¢ä¸åŒä½¿ç”¨è€…çš„è«‹æ±‚
    task_id = str(uuid.uuid4())
    # å®šç¾©æª”æ¡ˆè·¯å¾‘
    zip_save_path = os.path.join(UPLOAD_DIR, f"{task_id}_{file.filename}")
    extract_folder = os.path.join(EXTRACT_DIR, task_id)
    vector_db_folder = os.path.join(OUTPUT_DIR, task_id)
    output_zip_path = os.path.join(OUTPUT_DIR, f"{task_id}_faiss.zip")

    # å°‡ä½¿ç”¨è€…ä¸Šå‚³çš„ Zip å¯«å…¥ç¡¬ç¢Ÿ
    with open(zip_save_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    try:
        # 1. å‘¼å«å…±é€šå‡½æ•¸ï¼šè§£å£“ç¸®ä¸¦è®€å–æ‰€æœ‰æ–‡ä»¶
        all_documents = process_zip_to_docs(zip_save_path, extract_folder)
        # å¦‚æœæ²’æœ‰è®€å–åˆ°ä»»ä½•æ”¯æ´çš„æ–‡ä»¶ï¼Œå›å‚³ 400 éŒ¯èª¤
        if not all_documents:
            return JSONResponse(status_code=400, content={"error": "Zip å…§ç„¡æ”¯æ´çš„æ–‡ä»¶"})

        # 2. æ–‡å­—åˆ‡åˆ† (Chunking)
        # è¨­å®šåˆ‡åˆ†å™¨ï¼šæ¯å¡Š 1000 å­—å…ƒï¼Œé‡ç–Š 200 å­—å…ƒ
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = text_splitter.split_documents(all_documents)

        # 3. å‘é‡åŒ– (Embedding)
        # åˆå§‹åŒ– OpenAI Embeddings æ¨¡å‹ (ä½¿ç”¨ text-embedding-3-large)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)
        # ä½¿ç”¨ FAISS å°‡åˆ‡åˆ†å¾Œçš„æ–‡ä»¶è½‰æ›ç‚ºå‘é‡ä¸¦å»ºç«‹ç´¢å¼•
        vectorstore = FAISS.from_documents(split_docs, embeddings)

        # 4. å°‡å‘é‡è³‡æ–™åº«å­˜æª”
        # å°‡ FAISS ç´¢å¼•å„²å­˜åˆ°æœ¬åœ°è³‡æ–™å¤¾ (åŒ…å« index.faiss å’Œ index.pkl)
        vectorstore.save_local(vector_db_folder)

        # 5. æ‰“åŒ…æˆ Zip
        with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(vector_db_folder):
                for file in files:
                    # è¨ˆç®—ç›¸å°è·¯å¾‘ï¼Œä¿æŒè³‡æ–™å¤¾çµæ§‹å¯«å…¥ zip
                    arcname = os.path.relpath(os.path.join(root, file), os.path.join(vector_db_folder, '..'))
                    zipf.write(os.path.join(root, file), arcname)

        # 6. è¨­å®šèƒŒæ™¯æ¸…ç†ä»»å‹™ (åˆªé™¤æš«å­˜æª”)
        cleanup_targets_files = [zip_save_path, output_zip_path]
        cleanup_targets_dirs = [extract_folder, vector_db_folder]
        background_tasks.add_task(cleanup_files, cleanup_targets_files, cleanup_targets_dirs)

        # å›å‚³ç”Ÿæˆçš„ ZIP æª”çµ¦ä½¿ç”¨è€…ä¸‹è¼‰
        return FileResponse(output_zip_path, filename=f"faiss_db_{task_id[:8]}.zip", media_type='application/zip')

    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¸…ç†
        cleanup_files([zip_save_path], [extract_folder, vector_db_folder])
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# åŠŸèƒ½ 2: ä¸Šå‚³ä¸¦ç›´æ¥å•ç­” (æ”¯æ´ RAG Zip æˆ– åŸå§‹æ–‡ä»¶ Zip)
# =========================================================
@app.post("/ask_with_zip")
async def ask_with_zip(
    background_tasks: BackgroundTasks, # ç”¨æ–¼è¨­å®šèƒŒæ™¯æ¸…ç†ä»»å‹™
    question: str = Form(...),         # æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œ (Form Data)
    file: UploadFile = File(...)       # æ¥æ”¶ä½¿ç”¨è€…ä¸Šå‚³çš„æª”æ¡ˆ (Zip)
):
    """
    é€™æ˜¯ä¸»è¦çš„å•ç­” APIã€‚
    å®ƒæœƒè‡ªå‹•åµæ¸¬ä¸Šå‚³çš„ Zip æ˜¯ã€Œå·²ç¶“åšå¥½çš„ RAG è³‡æ–™åº«ã€é‚„æ˜¯ã€ŒåŸå§‹æ–‡ä»¶ã€ã€‚
    """
    # å–å¾— OpenAI API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return JSONResponse(status_code=500, content={"error": "æœªè¨­å®š OPENAI_API_KEY"})

    # ç”¢ç”Ÿå”¯ä¸€çš„ Task ID
    task_id = str(uuid.uuid4())
    # è¨­å®šæª”æ¡ˆå­˜æ”¾èˆ‡è§£å£“ç¸®è·¯å¾‘
    zip_save_path = os.path.join(UPLOAD_DIR, f"{task_id}_{file.filename}")
    extract_folder = os.path.join(EXTRACT_DIR, task_id)

    # å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ
    with open(zip_save_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    try:
        # é©—è­‰æ˜¯å¦ç‚º ZIP æ ¼å¼
        if not zipfile.is_zipfile(zip_save_path):
             return JSONResponse(status_code=400, content={"error": "ç„¡æ•ˆçš„ ZIP æª”"})

        # å…ˆé€²è¡Œè§£å£“ç¸®ï¼Œé€™æ¨£æˆ‘å€‘æ‰èƒ½æª¢æŸ¥è£¡é¢çš„å…§å®¹
        with zipfile.ZipFile(zip_save_path, 'r') as zip_ref:
            zip_ref.extractall(extract_folder)

        # === é—œéµé‚è¼¯ï¼šåµæ¸¬æ˜¯å¦ç‚º RAG è³‡æ–™åº« (FAISS) ===
        is_rag_db = False  # é è¨­ä¸æ˜¯ RAG DB
        db_folder = None   # ç”¨ä¾†å­˜æ”¾ index.faiss æ‰€åœ¨çš„è³‡æ–™å¤¾è·¯å¾‘
        
        # éæ­·è§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾ï¼Œå°‹æ‰¾ FAISS é—œéµæª”æ¡ˆ
        for root, _, files in os.walk(extract_folder):
            # å¦‚æœè³‡æ–™å¤¾å…§åŒæ™‚åŒ…å« index.faiss å’Œ index.pklï¼Œåˆ¤å®šç‚º RAG è³‡æ–™åº«
            if "index.faiss" in files and "index.pkl" in files:
                is_rag_db = True
                db_folder = root
                break  # æ‰¾åˆ°å¾Œå°±åœæ­¢æœå°‹
        
        # åˆå§‹åŒ– Embeddings æ¨¡å‹ (ä¸ç®¡æ˜¯å“ªç¨®æ¨¡å¼ï¼Œéƒ½éœ€è¦ç”¨å®ƒä¾†è™•ç†å•é¡Œå‘é‡)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)

        # æ ¹æ“šåµæ¸¬çµæœæ±ºå®šè™•ç†æµç¨‹
        if is_rag_db:
            # ã€æ¨¡å¼ Aã€‘ä½¿ç”¨è€…ä¸Šå‚³çš„æ˜¯ RAG Zip (å·²åšå¥½çš„è³‡æ–™åº«)
            print(f"åµæ¸¬åˆ° RAG è³‡æ–™åº«ï¼Œè¼‰å…¥è·¯å¾‘: {db_folder}")
            # ä½¿ç”¨ FAISS.load_local ç›´æ¥è¼‰å…¥è³‡æ–™åº«ï¼Œé€Ÿåº¦æ¥µå¿«
            # allow_dangerous_deserialization=True æ˜¯å¿…é ˆçš„ï¼Œå› ç‚ºæˆ‘å€‘æ­£åœ¨è¼‰å…¥ pickle æª”
            vectorstore = FAISS.load_local(
                db_folder, 
                embeddings, 
                allow_dangerous_deserialization=True
            )
        else:
            # ã€æ¨¡å¼ Bã€‘ä½¿ç”¨è€…ä¸Šå‚³çš„æ˜¯ åŸå§‹æ–‡ä»¶ Zip (PDF/Word ç­‰)
            print("æœªåµæ¸¬åˆ°è³‡æ–™åº«ï¼Œå˜—è©¦è®€å–åŸå§‹æ–‡ä»¶...")
            # å‘¼å«ä¹‹å‰çš„é‚è¼¯ï¼šè®€å–æ‰€æœ‰åŸå§‹æ–‡ä»¶
            all_documents = process_zip_to_docs(zip_save_path, extract_folder)
            
            # å¦‚æœ Zip è£¡æ—¢æ²’æœ‰ RAG DBï¼Œä¹Ÿæ²’æœ‰æ”¯æ´çš„åŸå§‹æ–‡ä»¶ï¼Œå ±éŒ¯
            if not all_documents:
                return JSONResponse(status_code=400, content={"error": "Zip å…§ç„¡æ”¯æ´çš„æ–‡ä»¶ (äº¦é RAG DB)"})

            # é€²è¡Œåˆ‡åˆ† (Chunking)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            split_docs = text_splitter.split_documents(all_documents)
            # ç¾å ´è£½ä½œå‘é‡è³‡æ–™åº« (In-Memory)
            vectorstore = FAISS.from_documents(split_docs, embeddings)

        # === å•ç­”æµç¨‹ (Retrieval & Generation) ===
        
        # å°‡å‘é‡åº«è½‰æ›ç‚ºæª¢ç´¢å™¨ (Retriever)ï¼Œè¨­å®šæœå°‹æœ€ç›¸é—œçš„ 5 å€‹ç‰‡æ®µ (k=5)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # å®šç¾© Prompt Template (æç¤ºè©æ¨¡æ¿)ï¼ŒæŒ‡å° AI å¦‚ä½•å›ç­”
        prompt_template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ•™ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ä¸Šä¸‹æ–‡å…§å®¹ä¾†å›ç­”å­¸ç”Ÿçš„å•é¡Œã€‚

        ä¸Šä¸‹æ–‡:
        {context}

        å•é¡Œ: {question}

        å›ç­”:"""

        # å»ºç«‹ LangChain çš„ Prompt ç‰©ä»¶
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        # å»ºç«‹ RetrievalQA Chain (æª¢ç´¢å•ç­”éˆ)
        qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(model="gpt-4o", temperature=0, api_key=api_key), # ä½¿ç”¨ GPT-4o æ¨¡å‹
            chain_type="stuff", # "stuff" æ¨¡å¼ï¼šæŠŠæ‰€æœ‰æª¢ç´¢åˆ°çš„å…§å®¹å¡é€² Prompt
            retriever=retriever, # ä½¿ç”¨å‰›å‰›è¨­å®šçš„æª¢ç´¢å™¨
            chain_type_kwargs={"prompt": PROMPT}, # å‚³å…¥è‡ªè¨‚çš„ Prompt
            return_source_documents=True # è¦æ±‚å›å‚³åƒè€ƒçš„ä¾†æºæ–‡ä»¶ï¼Œæ–¹ä¾¿é¡¯ç¤ºå‡ºè™•
        )

        # åŸ·è¡Œå•ç­”éˆï¼Œå‚³å…¥ä½¿ç”¨è€…çš„å•é¡Œ
        result = qa_chain.invoke({"query": question})

        # === æ¸…ç†èˆ‡å›å‚³ ===
        # è¨­å®šèƒŒæ™¯ä»»å‹™ï¼Œåˆªé™¤å‰›å‰›ä¸Šå‚³å’Œè§£å£“ç¸®çš„æš«å­˜æª”
        cleanup_targets_files = [zip_save_path]
        cleanup_targets_dirs = [extract_folder]
        background_tasks.add_task(cleanup_files, cleanup_targets_files, cleanup_targets_dirs)

        # å›å‚³ JSON çµæœ
        return {
            "question": question, # å›å‚³åŸå§‹å•é¡Œ
            "answer": result["result"], # å›å‚³ AI çš„å›ç­”
            # å›å‚³åƒè€ƒè³‡æ–™ä¾†æº (å»é‡è¤‡)ï¼Œå¦‚æœæœ‰ metadata å‰‡é¡¯ç¤ºæª”åï¼Œå¦å‰‡é¡¯ç¤º unknown
            "sources": list(set([doc.metadata.get('filename', 'unknown') for doc in result.get("source_documents", [])]))
        }

    except Exception as e:
        # å¦‚æœç™¼ç”Ÿä»»ä½•æœªé æœŸçš„éŒ¯èª¤ï¼Œä¹Ÿè¦æ¸…ç†æš«å­˜æª”
        cleanup_files([zip_save_path], [extract_folder])
        # å›å‚³ 500 éŒ¯èª¤èˆ‡è©³ç´°éŒ¯èª¤è¨Šæ¯
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# [æ–°å¢] API 3: æ™ºæ…§å‡ºé¡Œ (Generate Question)
# é‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ä¸Šå‚³çš„æª”æ¡ˆï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨ä¼ºæœå™¨ä¸Šçš„ 'rag_db.zip'
# =========================================================
@app.post("/api/generate_question")
async def generate_practice_question_with_upload(
    background_tasks: BackgroundTasks, # ç”¨æ–¼è¨­å®šèƒŒæ™¯æ¸…ç†ä»»å‹™
    file: Optional[UploadFile] = File(None), # [é—œéµä¿®æ”¹] æª”æ¡ˆç‚ºå¯é¸ (Optional)ï¼Œé è¨­ç‚º None
    qtype: str = Form(...),            # ä½¿ç”¨ Form Data æ¥æ”¶é¡Œå‹
    level: str = Form(...)             # ä½¿ç”¨ Form Data æ¥æ”¶é›£åº¦
):
    """
    ã€ä¸€æ¢é¾å‡ºé¡Œ APIã€‘
    1. æª¢æŸ¥æ˜¯å¦æœ‰ä¸Šå‚³ ZIPï¼Œè‹¥æœ‰å‰‡ä½¿ç”¨ã€‚
    2. è‹¥ç„¡ä¸Šå‚³ï¼Œæª¢æŸ¥ä¼ºæœå™¨åŒå±¤ç›®éŒ„ä¸‹æ˜¯å¦æœ‰ 'rag_db.zip'ï¼Œè‹¥æœ‰å‰‡ä½¿ç”¨ã€‚
    3. è‡ªå‹•è¼‰å…¥å‘é‡è³‡æ–™åº« -> æª¢ç´¢ Context -> åµæ¸¬æª”æ¡ˆåˆ—è¡¨ -> å‘¼å« GPT-4o ç”Ÿæˆé¡Œç›®ã€‚
    """
    # æª¢æŸ¥ API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key: return JSONResponse(status_code=500, content={"error": "æœªè¨­å®š OPENAI_API_KEY"})
    
    # åˆå§‹åŒ– OpenAI åŸç”Ÿå®¢æˆ¶ç«¯
    client = get_openai_client()
    
    # å»ºç«‹ä»»å‹™ ID èˆ‡è·¯å¾‘
    task_id = str(uuid.uuid4())
    extract_folder = os.path.join(EXTRACT_DIR, task_id)

    # === [é—œéµé‚è¼¯] æ±ºå®šä½¿ç”¨å“ªå€‹ ZIP æª”æ¡ˆä¾†æº ===
    zip_source_path = "" # ç”¨ä¾†å­˜æ”¾æœ€çµ‚è¦è§£å£“ç¸®çš„æª”æ¡ˆè·¯å¾‘
    
    if file:
        # æƒ…å¢ƒ A: ä½¿ç”¨è€…æœ‰ä¸Šå‚³æª”æ¡ˆ
        print(f"æ”¶åˆ°ä½¿ç”¨è€…ä¸Šå‚³æª”æ¡ˆ: {file.filename}")
        zip_source_path = os.path.join(UPLOAD_DIR, f"{task_id}_{file.filename}")
        with open(zip_source_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer) # å°‡ä¸Šå‚³çš„å…§å®¹å¯«å…¥ç¡¬ç¢Ÿ
    else:
        # æƒ…å¢ƒ B: ä½¿ç”¨è€…æ²’ä¸Šå‚³ï¼Œæª¢æŸ¥ä¼ºæœå™¨é è¨­æª”æ¡ˆ
        default_zip = "rag_db.zip" # é è¨­æª”æ¡ˆåç¨±
        if os.path.exists(default_zip):
            print(f"ä½¿ç”¨ä¼ºæœå™¨é è¨­æª”æ¡ˆ: {default_zip}")
            # è¤‡è£½ä¸€ä»½åˆ° upload è³‡æ–™å¤¾ï¼Œé¿å…å¤šåŸ·è¡Œç·’åŒæ™‚è®€å¯«åŒä¸€å€‹æª”æ¡ˆé€ æˆè¡çª
            zip_source_path = os.path.join(UPLOAD_DIR, f"{task_id}_default.zip")
            shutil.copy(default_zip, zip_source_path)
        else:
            # å…©è€…çš†ç„¡ï¼Œå›å‚³éŒ¯èª¤
            return JSONResponse(status_code=400, content={"error": "æœªä¸Šå‚³æª”æ¡ˆï¼Œä¸”ä¼ºæœå™¨æ‰¾ä¸åˆ°é è¨­çš„ rag_db.zip"})

    try:
        # é©—è­‰ ZIP æ ¼å¼
        if not zipfile.is_zipfile(zip_source_path):
             return JSONResponse(status_code=400, content={"error": "ç„¡æ•ˆçš„ ZIP æª”"})

        # 2. è§£å£“ç¸®
        with zipfile.ZipFile(zip_source_path, 'r') as zip_ref:
            zip_ref.extractall(extract_folder)

        # 3. åµæ¸¬ ZIP å…§å®¹ï¼šæ˜¯ã€Œå·²ç¶“åšå¥½çš„ FAISS DBã€é‚„æ˜¯ã€ŒåŸå§‹æ–‡ä»¶ã€ï¼Ÿ
        is_rag_db = False
        db_folder = None
        for root, _, files in os.walk(extract_folder):
            # æª¢æŸ¥æ˜¯å¦æœ‰ FAISS çš„ç´¢å¼•æª”æ¡ˆ
            if "index.faiss" in files and "index.pkl" in files:
                is_rag_db = True
                db_folder = root
                break
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)

        # 4. æº–å‚™ VectorStore (è¼‰å…¥æˆ–ç¾å ´è£½ä½œ)
        if is_rag_db:
            # æƒ…æ³ A: æ˜¯è™•ç†éçš„ rag.zip -> ç›´æ¥è¼‰å…¥ï¼Œé€Ÿåº¦å¿«
            vectorstore = FAISS.load_local(db_folder, embeddings, allow_dangerous_deserialization=True)
        else:
            # æƒ…æ³ B: æ˜¯åŸå§‹æ–‡ä»¶ Zip -> ç¾å ´åˆ‡åˆ†å‘é‡åŒ– (è¼ƒæ…¢)
            all_documents = process_zip_to_docs(zip_source_path, extract_folder)
            if not all_documents:
                return JSONResponse(status_code=400, content={"error": "Zip å…§ç„¡æ”¯æ´çš„è¬›ç¾©æ–‡ä»¶"})
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            split_docs = text_splitter.split_documents(all_documents)
            vectorstore = FAISS.from_documents(split_docs, embeddings)

        # 5. è‡ªå‹•åµæ¸¬æª”æ¡ˆåˆ—è¡¨ (File List) - è®“ AI çŸ¥é“æœ‰å“ªäº› GIS æª”æ¡ˆå¯ç”¨
        # ä½¿ç”¨è¼”åŠ©å‡½æ•¸æƒæè§£å£“ç¸®ç›®éŒ„
        file_names = get_gis_filenames(extract_folder)
        file_names_str = ", ".join(file_names) if file_names else "None"

        # 6. RAG æª¢ç´¢ (Retrieval) - æ‰¾å‡ºèˆ‡ã€Œé¡Œå‹/é›£åº¦ã€ç›¸é—œçš„å…§å®¹
        query = f"ç©ºé–“åˆ†æ {level} {qtype} é‡é»æ¦‚å¿µèˆ‡æ“ä½œæ­¥é©Ÿ"
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # æ’ˆå‰ 5 å€‹ç›¸é—œæ®µè½
        docs = retriever.invoke(query)
        context_text = "\n\n".join([d.page_content for d in docs]) # çµ„åˆ Context æ–‡å­—

        # 7. çµ„åˆ Prompt (System Prompt) - åš´æ ¼é™åˆ¶ AI è¡Œç‚º
        sys_role = "ä½ æ˜¯é ‚å°–çš„ç©ºé–“åˆ†æåŠ©æ•™ã€‚è«‹ä½¿ç”¨ GPT-4o çš„å¼·å¤§é‚è¼¯ä¾†å‡ºé¡Œã€‚"
        r_rules = """âš ï¸ åš´æ ¼é™åˆ¶ï¼š
1. å¯¦ä½œå…§å®¹å¿…é ˆé™å®šä½¿ç”¨ **R èªè¨€** (ä¾‹å¦‚ä½¿ç”¨ sf, terra, tmap, tidyverse ç­‰å¥—ä»¶)ã€‚
2. ğŸš« ç¦æ­¢æåŠ "ArcGIS", "QGIS" æˆ–é€šç”¨çš„ "GIS è»Ÿé«”" å­—çœ¼ã€‚
3. é¡Œç›®æ‡‰å¼•å°å­¸ç”Ÿå¯«å‡º R ç¨‹å¼ç¢¼ä¾†è§£æ±ºå•é¡Œã€‚
4. **è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) å‡ºé¡Œã€‚**"""
        
        system_instruction = f"""ä½ å¿…é ˆå¾æä¾›çš„ã€ŒçœŸå¯¦æª”æ¡ˆåˆ—è¡¨ã€ä¸­é¸æ“‡ä¸€å€‹æª”æ¡ˆä¾†è¨­è¨ˆæ“ä½œä»»å‹™ã€‚
çœŸå¯¦æª”æ¡ˆåˆ—è¡¨: [{file_names_str}]
(è‹¥é¸æ“‡ Shapefileï¼Œè«‹åªæåŠ .shp æª”ï¼Œä¸è¦æåŠ .dbf æˆ– .shx)
{r_rules}
ã€å‡ºé¡Œé‡è¦è¦ç¯„ã€‘
1. åœ¨ 'question_content' (é¡Œç›®) ä¸­ï¼šåªèªªæ˜**ä»»å‹™ç›®æ¨™**èˆ‡**ä½¿ç”¨è³‡æ–™**ã€‚âŒ åš´ç¦ç›´æ¥åˆ—å‡ºæ­¥é©Ÿ 1, 2, 3ã€‚è«‹ä¿ç•™æ€è€ƒç©ºé–“çµ¦å­¸ç”Ÿã€‚
2. åœ¨ 'hint' (æç¤º) ä¸­ï¼šæ‰åˆ—å‡ºè©³ç´°çš„è§£é¡Œæ­¥é©Ÿã€å»ºè­°ä½¿ç”¨çš„ R å¥—ä»¶èˆ‡å‡½æ•¸ã€‚"""

        task_instruction = f"ç›®å‰çš„é¡Œå‹ä»»å‹™æ˜¯ï¼šã€{qtype}ã€‘ã€‚é›£åº¦ï¼š{level}ã€‚"
        core_point = f"ğŸ”¥ **æœ¬æ¬¡é¡Œç›®æ ¸å¿ƒè€ƒé»ï¼šè«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè¬›ç¾©å…§å®¹è¨­è¨ˆ**"

        final_system_prompt = f"""
{sys_role}
{task_instruction}
{core_point}
(Please design the question around the core concept above.)
{system_instruction}
è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
{{ "question_content": "Question content (Markdown)...", "hint": "Hint for students...", "target_filename": "AIé¸æ“‡çš„æª”æ¡ˆåç¨±" }}
"""
        user_prompt_text = f"åƒè€ƒè¬›ç¾©å…§å®¹ï¼š\n{context_text}"

        # 8. å‘¼å« GPT-4o ç”Ÿæˆé¡Œç›®
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": final_system_prompt}, 
                {"role": "user", "content": user_prompt_text}
            ],
            response_format={"type": "json_object"}, # å¼·åˆ¶å›å‚³ JSON
            temperature=0.7 # ä¿æŒä¸€é»å‰µé€ åŠ›
        )
        
        # è¨­å®šèƒŒæ™¯æ¸…ç†ä»»å‹™ (åˆªé™¤æš«å­˜æª”ï¼ŒåŒ…å«è¤‡è£½å‡ºä¾†çš„ zip å’Œè§£å£“è³‡æ–™å¤¾)
        background_tasks.add_task(cleanup_files, [zip_source_path], [extract_folder])
        
        # å›å‚³ç”Ÿæˆçš„ JSON
        return json.loads(response.choices[0].message.content)

    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¸…ç†
        cleanup_files([zip_source_path], [extract_folder])
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# [æ–°å¢] API 4: æ™ºæ…§è©•åˆ† (Grade Submission)
# é‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ä¸Šå‚³çš„æª”æ¡ˆï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨ä¼ºæœå™¨ä¸Šçš„ 'rag_db.zip'
# =========================================================
@app.post("/api/grade_submission")
async def grade_submission_with_upload(
    background_tasks: BackgroundTasks,
    file: Optional[UploadFile] = File(None), # [é—œéµä¿®æ”¹] æª”æ¡ˆç‚ºå¯é¸ (Optional)
    question_text: str = Form(...),    # é¡Œç›®å…§å®¹
    student_answer: str = Form(...),   # å­¸ç”Ÿå›ç­”
    qtype: str = Form(...)             # é¡Œå‹
):
    """
    ã€è©•åˆ† APIã€‘
    1. ä¾æ“šä¸Šå‚³æˆ–é è¨­çš„ ZIP æº–å‚™è©•åˆ†æ¨™æº–åº«ã€‚
    2. æ ¹æ“šã€Œé¡Œç›®å…§å®¹ã€æª¢ç´¢ç›¸é—œè¬›ç¾© (Context)ã€‚
    3. å‘¼å« GPT-4o é€²è¡Œè©•åˆ†ã€‚
    """
    # æª¢æŸ¥ API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key: return JSONResponse(status_code=500, content={"error": "æœªè¨­å®š OPENAI_API_KEY"})

    client = get_openai_client()
    task_id = str(uuid.uuid4())
    extract_folder = os.path.join(EXTRACT_DIR, task_id)

    # === [é—œéµé‚è¼¯] æ±ºå®šä½¿ç”¨å“ªå€‹ ZIP æª”æ¡ˆä¾†æº ===
    zip_source_path = ""
    
    if file:
        print(f"æ”¶åˆ°ä½¿ç”¨è€…ä¸Šå‚³è©•åˆ†åƒè€ƒæª”: {file.filename}")
        zip_source_path = os.path.join(UPLOAD_DIR, f"{task_id}_{file.filename}")
        with open(zip_source_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    else:
        default_zip = "rag_db.zip"
        if os.path.exists(default_zip):
            print(f"ä½¿ç”¨ä¼ºæœå™¨é è¨­æª”æ¡ˆè©•åˆ†: {default_zip}")
            zip_source_path = os.path.join(UPLOAD_DIR, f"{task_id}_default.zip")
            shutil.copy(default_zip, zip_source_path) # è¤‡è£½ä¸€ä»½ï¼Œç¢ºä¿åŸ·è¡Œç·’å®‰å…¨
        else:
            return JSONResponse(status_code=400, content={"error": "æœªä¸Šå‚³æª”æ¡ˆï¼Œä¸”ä¼ºæœå™¨æ‰¾ä¸åˆ°é è¨­çš„ rag_db.zip"})

    try:
        if not zipfile.is_zipfile(zip_source_path):
             return JSONResponse(status_code=400, content={"error": "ç„¡æ•ˆçš„ ZIP æª”"})

        # 2. è§£å£“ç¸®
        with zipfile.ZipFile(zip_source_path, 'r') as zip_ref:
            zip_ref.extractall(extract_folder)

        # 3. åµæ¸¬ä¸¦è¼‰å…¥å‘é‡è³‡æ–™åº«
        is_rag_db = False
        db_folder = None
        for root, _, files in os.walk(extract_folder):
            if "index.faiss" in files and "index.pkl" in files:
                is_rag_db = True
                db_folder = root
                break
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)

        if is_rag_db:
            vectorstore = FAISS.load_local(db_folder, embeddings, allow_dangerous_deserialization=True)
        else:
            all_documents = process_zip_to_docs(zip_source_path, extract_folder)
            if not all_documents:
                return JSONResponse(status_code=400, content={"error": "Zip å…§ç„¡æ”¯æ´çš„è¬›ç¾©æ–‡ä»¶"})
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            split_docs = text_splitter.split_documents(all_documents)
            vectorstore = FAISS.from_documents(split_docs, embeddings)

        # 4. RAG æª¢ç´¢ (Retrieval) - ç”¨ã€Œé¡Œç›®ã€å»æ’ˆå‡ºã€Œæ¨™æº–ç­”æ¡ˆ/ç›¸é—œæ¦‚å¿µã€ä½œç‚ºè©•åˆ†ä¾æ“š (Context)
        # é€™æ¨£ AI æ‰èƒ½æ ¹æ“šè¬›ç¾©å…§å®¹è©•åˆ†ï¼Œè€Œä¸åªæ˜¯æ ¹æ“šé€šç”¨çŸ¥è­˜
        query = question_text
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        docs = retriever.invoke(query)
        context_text = "\n\n".join([d.page_content for d in docs])

        # 5. åˆ¤æ–·é¡Œå‹ä¸¦è¨­å®š Prompt
        is_conceptual = any(k in qtype for k in ["ç°¡ç­”", "Short Answer", "Intro"])
        
        if is_conceptual:
            # æƒ…å¢ƒ Aï¼šè§€å¿µç°¡ç­”é¡Œ Prompt
            prompt = f"""ä½ æ˜¯ä¸€ä½ç©ºé–“åˆ†æåŠ©æ•™ã€‚è«‹æ‰¹æ”¹é€™é“**ã€Œè§€å¿µç°¡ç­”é¡Œã€**ã€‚
ç›®æ¨™ï¼šè©•ä¼°å­¸ç”Ÿå° GIS åŸç†çš„ç†è§£ã€é‚è¼¯æ¨æ¼”èˆ‡è§£é‡‹æ¸…æ™°åº¦ã€‚
ã€é‡è¦é™åˆ¶ã€‘
1. **è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) æ’°å¯«æ‰€æœ‰è©•èªã€å„ªé»ã€å¼±é»èˆ‡è¡Œå‹•å»ºè­°ã€‚**
2. è‹¥åƒè€ƒè³‡æ–™ç‚ºè‹±æ–‡ï¼Œè«‹è‡ªè¡Œç¿»è­¯ä¸¦å…§åŒ–æˆä¸­æ–‡å›é¥‹ã€‚
ã€è©•åˆ†æ¨™æº–ã€‘A) æ¦‚å¿µæ­£ç¢ºæ€§ (3åˆ†), B) é‚è¼¯èˆ‡è§£é‡‹ (4åˆ†), C) å®Œæ•´æ€§ (3åˆ†)ã€‚
ã€è¼¸å‡º JSONã€‘{{ "score": int, "level": str, "rubric": [], "strengths": [], "weaknesses": [], "missing_items": [], "action_items": [] }}
[é¡Œç›®] {question_text}
[å­¸ç”Ÿå›ç­”] {student_answer}
[è¬›ç¾©ä¾æ“š] {context_text}"""
        else:
            # æƒ…å¢ƒ Bï¼šå¯¦ä½œé¡Œ Prompt (é è¨­)
            prompt = f"""ä½ æ˜¯ä¸€ä½ç©ºé–“åˆ†æåŠ©æ•™ã€‚è«‹æ‰¹æ”¹é€™é“**ã€ŒR èªè¨€å¯¦ä½œé¡Œã€**ã€‚
ç›®æ¨™ï¼šè©•ä¼° R ç¨‹å¼ç¢¼çš„æ­£ç¢ºæ€§ã€å¯é‡ç¾æ€§èˆ‡ç©ºé–“é‚è¼¯ã€‚
ã€é‡è¦é™åˆ¶ã€‘
1. **è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) æ’°å¯«æ‰€æœ‰è©•èªã€å„ªé»ã€å¼±é»èˆ‡è¡Œå‹•å»ºè­°ã€‚**
2. è‹¥åƒè€ƒè³‡æ–™ç‚ºè‹±æ–‡ï¼Œè«‹è‡ªè¡Œç¿»è­¯ä¸¦å…§åŒ–æˆä¸­æ–‡å›é¥‹ã€‚
ã€è©•åˆ†æ¨™æº–ã€‘A) éœ€æ±‚è¦†è“‹ (3åˆ†), B) ç©ºé–“é‚è¼¯ (4åˆ†), C) R ç¨‹å¼åš´è¬¹åº¦ (3åˆ†)ã€‚
ã€è¼¸å‡º JSONã€‘{{ "score": int, "level": str, "rubric": [], "strengths": [], "weaknesses": [], "missing_items": [], "action_items": [] }}
[é¡Œç›®] {question_text}
[å­¸ç”Ÿå›ç­”] {student_answer}
[è¬›ç¾©ä¾æ“š] {context_text}"""

        # 6. å‘¼å« OpenAI é€²è¡Œè©•åˆ†
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3 # è©•åˆ†å»ºè­°ä½¿ç”¨ä½æº«åº¦ï¼Œä¿æŒå®¢è§€ä¸€è‡´
        )
        
        # è¨­å®šèƒŒæ™¯æ¸…ç†ä»»å‹™
        background_tasks.add_task(cleanup_files, [zip_source_path], [extract_folder])
        
        # å›å‚³ JSON çµæœ
        return json.loads(response.choices[0].message.content)

    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¸…ç†
        cleanup_files([zip_source_path], [extract_folder])
        return JSONResponse(status_code=500, content={"error": str(e)})